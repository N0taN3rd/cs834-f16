\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{pdfpages}
\usepackage[english]{babel}
\usepackage [autostyle, english = american]{csquotes}
\usepackage{mathtools}
\usepackage{float}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{fancyvrb}
\usepackage{caption,subcaption}
%\usepackage{subfigure}
\usepackage[margin=0.6in]{geometry}
\usepackage{adjustbox}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{wrapfig}
\usepackage{newfloat}
\usepackage[cm]{fullpage}
\usepackage[cachedir=build,newfloat,outputdir=build]{minted}
\usepackage{verbatim}

\definecolor{mintedbackground}{rgb}{0,0,0}
\usemintedstyle{tango}
\newenvironment{code}{\captionsetup{type=listing}}{}
\SetupFloatingEnvironment{listing}{name=Source Code}
\captionsetup[subfigure]{subrefformat=simple,labelformat=simple}
\renewcommand{\thelisting}{\arabic{listing}}
\renewcommand\thesubfigure{(\alph{subfigure})}
\definecolor{lightgray}{rgb}{.7,.7,.7}
\definecolor{gray}{rgb}{.4,.4,.4}
\definecolor{darkblue}{rgb}{0,0,.3}
\definecolor{gray}{rgb}{0.4,0.4,0.4}
\definecolor{darkblue}{rgb}{0.0,0.0,0.6}
\definecolor{cyan}{rgb}{0.0,0.6,0.6}
\renewcommand{\thelisting}{\arabic{listing}}
\renewcommand\thesubfigure{(\alph{subfigure})}

\definecolor{termback}{HTML}{DBE0E0}
\definecolor{termkeyword}{HTML}{50DA8B}
\lstdefinestyle{Bash}
{language=bash,
keywordstyle=\color{termkeyword},
basicstyle=\ttfamily,
morekeywords={@.},
alsoletter={:~\$.},
morekeywords=[2]{peter@kbpet:},
keywordstyle=[2]{\color{termkeyword}},
literate={\$}{{\textcolor{termkeyword}{\$}}}1 
         {:}{{\textcolor{termkeyword}{:}}}1
         {~}{{\textcolor{termkeyword}{\textasciitilde}}}1,
}

\geometry{
 a4paper,
 total={170mm,257mm},
 left=10mm,
 right=10mm,
 top=10mm,
 bottom=15mm
}
\graphicspath{ {code/} }

\lstset{
  basicstyle=\ttfamily,
  columns=fullflexible,
  showstringspaces=false,
  commentstyle=\color{gray}\upshape
}

\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=blue
}
 \newmintedfile[pycode]{python3}{
frame=lines,
framesep=2mm,
fontsize=\footnotesize,
showtabs =false,
autogobble=true,
breaklines=true,
mathescape=true
}
 \newmintedfile[shellcode]{bash}{
frame=lines,
framesep=2mm,
fontsize=\footnotesize,
showtabs =false,
autogobble=true,
breaklines=true,
mathescape=true
}

\newmintedfile[rcode]{S}{
frame=lines,
framesep=2mm,
fontsize=\footnotesize,
showtabs =false,
autogobble=true,
breaklines=true,
mathescape=true
}
\newmintinline[ibash]{bash} {
}
\RecustomVerbatimCommand{\VerbatimInput}{VerbatimInput}%
{fontsize=\footnotesize,
 %
 frame=lines,  % top and bottom rule only
 framesep=2em, % separation between frame and text
 %
 labelposition=topline,
 %
 commandchars=\|\(\), % escape character and argument delimiters for
                      % commands within the verbatim
 commentchar=*        % comment character
}

\title{Assignment 3 \\ Introduction to Information Retrieval \\ CS734/834}
\author{John Berlin}
\date{\today}
\renewcommand\thesection{Q.\arabic{section}}
\renewcommand\thesubsection{\thesection}
\begin{document}
\maketitle
\newpage
\section*{Note}
As was mentioned in the this section for assignment two the size considerations for output files apply generated for this assignment.
The trec files produces by \textit{batch-search} and the indexes created by \textit{build} functions of galago prohibit them from inclusion in this report. Also used in this report the shell script \textit{rungalago.sh}, \autoref{code:rg}, was used to streamline the usage of galago by providing the argument boiler plate to command which required more than path arguments and to run the bin script for galago which resides in the nested directory structure of the application.  The two commands which required the argument boiler plate are build and build-window both create a search index whereas the latter creates a windowed version of the first. \newline 
The version of galago used in this report is 3.10, which was rebuilt from source, whereas the version available via the link found on the textbooks \href{http://www.search-engines-book.com/}{web-site} is version 1.04 built in 2009. Even tho galago is a Java application, it was found that this version suffered from performance issues due to its age. The biggest difference in 3.10 from 1.04 is that batch queries are in JSON format not XML and all other changes are minor feature additions and performance increases.
\newpage
\section{Question 6.1} \label{q1}
\begin{verbatim}
6.1. Using the Wikipedia collection provided at the book website, create a sample
of stem clusters by the following process:
1. Index the collection without stemming.
2. Identify the first 1,000 words (in alphabetical order) in the index.
3. Create stem classes by stemming these 1,000 words and recording which
words become the same stem.
4. Compute association measures (Diceâ€™s coefficient) between all pairs of stems
in each stem class. Compute co-occurrence at the document level.
5. Create stem clusters by thresholding the association measure. All terms that
are still connected to each other form the clusters.
Compare the stem clusters to the stem classes in terms of size and the quality (in
your opinion) of the groupings.
\end{verbatim}
\subsection{Answer} 
The first step was to create the index for the Wikipedia collection by executing the command \newline \ibash{./rungalago.sh build index htmls} where the first argument index is the location to where the index is to be placed and htmls is the location of the input. The html files of the Wikipedia collection were moved to a single directory as galago took some time to traverse the original directory structure of this data set which contained 6,042 files. As was mentioned in the note section the build command required argument boilerplate and was executed with the following addition arguments supplied by the script: \newline \ibash{--nonStemmedPosting=true --stemmedPostings=true --stemmer+krovetz --corpus=true}. By passing the arguments nonStemmedPostings and stemmedPostings galago created separate index files for both index types. Indexing the collection was surprisingly quick as shown in the report generated once indexing has finished \autoref{fig:q1idxreport}. After the build was complete the python file indexer.py, \autoref{code:idxr}, was used to dump the index file and extract terms.
\begin{figure}[H]
\centering
\includegraphics[scale=0.8]{indexingReport.png}
\caption{Indexing Report}
\label{fig:q1idxreport}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{q1Run.png}
\caption{Python Output}
\label{fig:q1Run}
\end{figure}
\noindent After the index was built the python file q1\_6dot1run.py, \autoref{code:scd}, was used to generate the results for this question. Running the file produced the output seen in \autoref{fig:q1Run}. This python script expects the directories \textit{output\_files} and \textit{pickled} to be present in the working directory the script is executed at using \ibash{python3 q1_6dot1run.py}. The first 1000 words in the collection only began with the letter \textit{a} which I found to lead to un-interesting results. To augment this I stemmed the entire index using four different stemmers: Porter, Snowball, WordnetLemmatizer, and Lancaster which are available via the nltk.stem package. This was achieved by using the pseq (parallel sequence)  function from the pyfunctional package which made the processing timely. Processing the data used the \textit{flatmap} function in parallel to transform the was utilized to transform the list of tuples (StemmerName,Stem,Word) to single list then foldleft was used to accumulate the tuples into Stemmer[Stem][words] structure. After which the stem classes for a stemmer were written to files with name idx\_[Stemmer].txt .  \newline \newline \noindent
Computing Dice's coefficient and co-occurrence at document level was done by constructing queries run via galago threaded-batch-search. After the queries were written and serialized to disk after which they were executed via the python code for this question and the trec file format returned by galago were also written to disk. A total of 1,453,423 queries were created of which Lancaster had 694,985, WordNetLemmatizer had 184,843, Porter had 286,350, and  Snowball had 287,245. Calculation for Dice's coefficient required a slight modification due to the combine operator being an \href{https://sourceforge.net/p/lemur/wiki/Belief Operations/}{or operation} which was to subtract the counts for a and b from the addition of the individual counts for the word pairs. The filter threshold was set at $0.001$ due to the relatively small number of documents in this collection and each stemmers results were written to files with name Stemmer\_dice.txt and Stemmer\_dice\_filtered.txt. The final step was to generate the report for the new stem classes \autoref{q1:report}.
\newline \newline \noindent To my surprise each stem class created each one contained the same number of words as the original stemming contained. The scores seen in \autoref{q1:report} are from the cumulative scores for all stems classes which was calculated by taking the sum of the each word pairs Dice score and dividing it by the number of pairs contained in the new stem class if the stem class had more than one pair. Also of note the stems not in the new stem classes the number of words for each was one to two.
\lstinputlisting[caption=Stem Class Dice Filtered Report, label=q1:report]{code/q1report.txt}
Now to ensure this was not off I looked for the stem \textit{admir} in all the generated classes. All but WordNetLemmatizer contained this stem as seen in \autoref{q1:rsame}.
\lstinputlisting[breaklines=true,caption=Stem Class Dice Filtered Report, label=q1:rsame]{code/same.txt}
But when grepping for that stem, \autoref{fig:grep}, it is clear to see WordNet does not do what one might think if not looking into the kind of stemming it does. For instance \textit{/do do dos} the lemmatization works well. But for \textit{/doje doje} and \textit{/dojo dojo} it does not as both would be expected to stem to \textit{doj} at least. But lemmatisation \footnote{https://en.wikipedia.org/wiki/Lemmatisation} is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form  
\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{grepOutput.png}
\caption{Grep WordNet admir}
\label{fig:grep}
\end{figure}
\newpage
\begin{code}
\captionof{listing}{Extract Terms From Index}
 \label{code:idxr}
	\pycode{code/indexer.py}
\end{code}
\begin{code}
\captionof{listing}{Stem Cluser Generation Dice} 
\label{code:scd}
	\pycode{code/q1_6dot1.py}
\end{code}
\newpage
\section{MLN1}
\begin{verbatim}
using the small wikipedia example, choose 10 words 
and create stem classes as per the algorithm on pp. 191-192
\end{verbatim}
\subsection{Answer}
This question builds heavily on the work done in question one and uses the work done by \autoref{q1} up to the generation of the stem classes which is where I will focus on in answering this question except that the queries used \#uw:50 as the operator for the co-occurrence query. The code for this question can been seen in \autoref{code:gsc}.  Like \autoref{q1} all stemmers used in it were considered when creating the graphs. The graphs were creating using networkx library and the edges were stem to word and word to stem to ensure the connection. Both connected components and the strong connected components directed graph counterpart were generated. In order to make presenting the results of this work I combined the connected and strongly connected data points into a single graph per stemmer. The code used to produce the graphs graphStemClass.py can bee seen in  \autoref{code:gsc} and the R code stemClassClusterReport.R can bee seen in \autoref{code:sccgr}. \newline \newline \noindent 
The colors of each graph represent the percentage of how well these new stem class fair in comparison to the old. For instance when the label states 100\% fully covered it means that the new stem class contained the original stem i.e /demi and all the words covered by the original likewise 100\% only stem means the new stem class only contained the stem not any of the words contained in the old. All other percentages represent the fractional amount of the previous two measures. The different graph types are encoded in the shape attribute with the y axis representing how many times a stem class occurred for the x axis which is the size of the class i.e how many words are contained in it. 
\newline \newline \noindent  As seen in \autoref{fig:lanc} lancaster, \autoref{fig:portc} Porter, and \autoref{fig:snbc} Snowball the new stem classes contained more words than the original and the graphs generated take the shape of a power law distribution. The Porter and Snowball stemmer had the least number of new stem classes created and generally had less of the original stem classes words. As stated in the answer to  \autoref{q1} the WordNetLemmatizer, \autoref{fig:wrdnc}, performed the worst out of the four.
\begin{figure}[H]
\centering
\includegraphics[scale=0.8]{Lancasterconnected.png}
\caption{Lancaster Connected}
\label{fig:lanc}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[scale=0.8]{PorterStemmerconnected.png}
\caption{Porter Connected}
\label{fig:portc}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[scale=0.8]{SnowballStemmerconnected.png}
\caption{Snowball Connected}
\label{fig:snbc}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[scale=0.8]{WordNetLemmatizerconnected.png}
\caption{WordNet Lemmatizer Connected}
\label{fig:wrdnc}
\end{figure}
\begin{code}
\captionof{listing}{Stem Class From Graph Using Ordered Window} 
\label{code:gsc}
	\pycode{code/graphStemClass.py}
\end{code}
\begin{code}
\captionof{listing}{Stem Class Cluster Graphs}
 \label{code:sccgr}
	\rcode{code/stemClassClusterReport.R}
\end{code}
\newpage
\section{MLN2}
\begin{verbatim}
using the small wikipedia example, choose 10 words
and compute MIM, EMIM, chi square, dice association
measures for full document & 5 word windows (cf. pp. 203-
205)
\end{verbatim}
\subsection{Answer}
In order to complete this question a 5 word window was created using \newline \ibash{./rungalago.sh build-window 5 false index3 htmls} where false was to create unordered window. The python file associations.py \autoref{code:associations} was used to generate the results which were over the all the words contained in the wikipedia collection. \autoref{tbl:10} shows the results after I hand picked ten most interesting ones of which 5 were looked for using grep. The first word rascal has high dice and mi with dizzee but the context in which this happens is unknown. Sportscar is highly associated with boxster along Ozzy Ozbourne and Lynyrd Skynyrd both of which I was unsure would be in this collection. Coder well robocoder has an association with lesfer this association I am completely unsure off. The others were the most recognizable pairs when glancing in the file. I am for sure that the $chi^2$ measure was completely miscalculated along with emi. I can only attribute this to human error or the number of occurrences to number of words was so small it threw off the calculations.
\begin{table}[H]
\centering
\caption{10 Word Associations}
\label{tbl:10}
\begin{tabular}{llllll}
w1 & w2 & dice & mi & emi & chi2 \\
dizzee & rascal & 1.0 & 1.0 & 0.0000017097 & 88184514016920.98 \\
sportscar & boxster & 1.00000 & 1.00000 & 0.0000017097 & 88184514016920.98 \\
ozzy & osbourne & 1.00000 & 0.50000 & 0.0000032718 & 88184514016920.98 \\
lynyrd & skynyrd & 1.00000 & 0.50000 & 0.0000032718 & 88184514016920.98 \\
jeepers & creepers & 1.00000 & 1.00 & 0.0000017097 & 88184514016920.98 \\
robocoder & lesfer & 0.50000 & 0.125 & 0.0000029765 & 22046128504230.24 \\
spiffy & sperry & 0.42857 & 0.0375 & 0.0000081603 & 19841515653807.22 \\
pyrokinetic & thermokinetic & 0.40 & 0.16667 & 0.0000015189 & 14697419002820.16 \\
friends & video & 0.00068 & 0.00 & 0.0000002062 & 49448444.52 \\
career & research & 0.00068 & 0.00 & 0.0000000783 & 45144302.65
\end{tabular}
\end{table}
\begin{code}
\captionof{listing}{Association Measures} 
\label{code:associations}
	\pycode{code/associations.py}
\end{code}
\newpage
\section{Question 6.2}
\begin{verbatim}
6.2. Create a simple spelling corrector based on the noisy channel model. Use a
single-word language model, and an error model where all errors with the same
edit distance have the same probability. Only consider edit distances of 1 or 2.
Implement your own edit distance calculator (example code can easily be found
on the Web).
\end{verbatim}
\subsection{Answer}
The correct spelling of words was provides using the brown, gutenberg,reuters, ingaugural and words corpus's available from the nltk library giving a total of 285,673 words. Calculating the edit distance was done using the Damerau Levenshtein distance (my third implementation, the first two were native C++ implementations for \href{https://github.com/N0taN3rd/Native-To-Java-StringUtils}{Java}). I must confess I am not sure I meet the error model requirement but make up for that in the search space. Finding the correction is done by getting the edit distance for 285,673 words to the misspelled word and then filtering the weights to keep only those who are within a distance of two all in parallel. The probability calculation is the \mintinline{python3}{within2Word / sum(wordCounts)}. The correction is the maximum probability of all the words remaining after filtering, if no words are returned \mintinline{python3}{'No Spellz Korrektion Found'} is returned ;) \newline The code for this noisy.py \autoref{code:noise}.
Running the code \ibash{python3 noisy.py --wordz korrectud lov} produced the following output \autoref{fig:noise}.
\begin{figure}[H]
\centering
\includegraphics[scale=0.8]{noisy.png}
\caption{Noisy Spell Corrector Run}
\label{fig:noise}
\end{figure}
\begin{code}
\captionof{listing}{Noisy Spell Corrector} 
\label{code:noise}
	\pycode{code/noisy.py}
\end{code}
\section{Not Attempted}
\begin{code}
\captionof{listing}{Run Galago}
 \label{code:rg}
	\shellcode{code/rungalago.sh}
\end{code}
\end{document}