\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{pdfpages}
\usepackage[english]{babel}
\usepackage [autostyle, english = american]{csquotes}
\usepackage{mathtools}
\usepackage{float}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{fancyvrb}
\usepackage{caption,subcaption}
%\usepackage{subfigure}
\usepackage[margin=0.6in]{geometry}
\usepackage[export]{adjustbox}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{wrapfig}
\usepackage{newfloat}
\usepackage[cm]{fullpage}
\usepackage[cachedir=build,newfloat,outputdir=build]{minted}
\usepackage{verbatim}
\usepackage{tikz}
\usetikzlibrary{decorations.pathmorphing,positioning}

\tikzset{mydeco/.style={decoration={random steps,segment length=.08em,amplitude=.05em},decorate,line cap=round}}

\tikzset{anarchy/.pic={
    \draw[line width=.1em,mydeco,decorate,red,fill=red](0em,0em)--(1.2em,.3em)--cycle;
    \draw[line width=.1em,mydeco,red,fill=red](.2em,-.3em)--(.5em,.7em)--cycle;
    \draw[line width=.1em,mydeco,red,fill=red](.5em,.7em)--(.8em,-.3em)--cycle;
    \draw[line width=.1em,red,line cap=round,mydeco](.5em,.2em) circle(.6em);
}}

\newcommand{\anarchy}{\tikz\pic{anarchy};}

\definecolor{mintedbackground}{rgb}{0,0,0}
\usemintedstyle{tango}
\newenvironment{code}{\captionsetup{type=listing}}{}
\SetupFloatingEnvironment{listing}{name=Source Code}
\captionsetup[subfigure]{subrefformat=simple,labelformat=simple}
\renewcommand{\thelisting}{\arabic{listing}}
\renewcommand\thesubfigure{(\alpha{subfigure})}
\definecolor{lightgray}{rgb}{.7,.7,.7}
\definecolor{gray}{rgb}{.4,.4,.4}
\definecolor{darkblue}{rgb}{0,0,.3}
\definecolor{gray}{rgb}{0.4,0.4,0.4}
\definecolor{darkblue}{rgb}{0.0,0.0,0.6}
\definecolor{cyan}{rgb}{0.0,0.6,0.6}
\renewcommand{\thelisting}{\arabic{listing}}
\renewcommand\thesubfigure{(\alph{subfigure})}

\definecolor{termback}{HTML}{DBE0E0}
\definecolor{termkeyword}{HTML}{50DA8B}
\lstdefinestyle{Bash}
{language=bash,
keywordstyle=\color{termkeyword},
basicstyle=\ttfamily,
morekeywords={@.},
alsoletter={:~\$.},
morekeywords=[2]{peter@kbpet:},
keywordstyle=[2]{\color{termkeyword}},
literate={\$}{{\textcolor{termkeyword}{\$}}}1 
         {:}{{\textcolor{termkeyword}{:}}}1
         {~}{{\textcolor{termkeyword}{\textasciitilde}}}1,
}

\geometry{
 a4paper,
 total={170mm,257mm},
 left=10mm,
 right=10mm,
 top=10mm,
 bottom=15mm
}
\graphicspath{ {images/} }

\lstset{
  basicstyle=\ttfamily,
  columns=fullflexible,
  showstringspaces=false,
  commentstyle=\color{gray}\upshape
}

\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=blue
}
 \newmintedfile[pycode]{python3}{
frame=lines,
framesep=2mm,
fontsize=\footnotesize,
showtabs =false,
autogobble=true,
breaklines=true,
mathescape=true
}
 \newmintedfile[shellcode]{bash}{
frame=lines,
framesep=2mm,
fontsize=\footnotesize,
showtabs =false,
autogobble=true,
breaklines=true,
mathescape=true
}

\newmintedfile[rcode]{S}{
frame=lines,
framesep=2mm,
fontsize=\footnotesize,
showtabs =false,
autogobble=true,
breaklines=true,
mathescape=true
}
\newmintinline[ibash]{bash} {
}

\newmintinline[ipy]{python3} {
}

\newmintinline[ir]{S} {
}
\RecustomVerbatimCommand{\VerbatimInput}{VerbatimInput}%
{fontsize=\footnotesize,
 %
 frame=lines,  % top and bottom rule only
 framesep=2em, % separation between frame and text
 %
 labelposition=topline,
 %
 commandchars=\|\(\), % escape character and argument delimiters for
                      % commands within the verbatim
 commentchar=*        % comment character
}

\newcommand*{\escape}[1]{\texttt{\textbackslash#1}}
\newcommand{\dfa}[3]{\ir{#1#2#3}}

\newcommand{\rpm}{\sbox0{$1$}\sbox2{$\scriptstyle\pm$}
  \raise\dimexpr(\ht0-\ht2)/2\relax\box2 }

\title{Assignment 5 \\ Introduction to Information Retrieval \\ CS734/834}
\author{John Berlin}
\date{\today}
\renewcommand\thesection{Q.\arabic{section}}
\renewcommand\thesubsection{\thesection}
\begin{document}
\maketitle
\newpage
\section*{Note}
In order to making the computation of the iterations for Hits and Pagerank stop at the desired iteration for \autoref{q1}, I modified the respective algorithm implementations that the NetworkX\footnote{\href{http://networkx.readthedocs.io/en/networkx-1.10/}{http://networkx.readthedocs.io/en/networkx-1.10/}} library provides. The modifications were only done so that they do not throw an exception if convergence was not reached when the stopping point parameter \textit{max\_iter} was hit.  These changes can be seen in \autoref{code:nxc}. Also utilized in \autoref{q1} are three new context classes were created GraphBuilder,AutoSLatexTable, and AutoSaveTwoFileTypes. GraphBuilder creates a new networkx Graph and populates it from the given configuration. 
AutoSLatexTable is similar to the other autosaver classes discussed in this section for Q4 but it creates a latex table utilizing the tabulate library\footnote{\href{https://pypi.python.org/pypi/tabulate}{https://pypi.python.org/pypi/tabulate}}. AutoSaveTwoFileTypes allows usage of two autosavers from a given configuration containing the required arguments for each.
\newpage
\section{Question 10.3} \label{q1}
\begin{verbatim}
Compute five iterations of HITS (see Algorithm 3) and PageRank (see Fig-
ure 4.11) on the graph in Figure 10.3. Discuss how the PageRank scores compare
to the hub and authority scores produced by HITS
\end{verbatim}
\subsection*{Answer} 
The networkx library was utilized to answer this question and node,edges files used when creating the graphs can be found in the data folder that companies this report. After computing each iteration the respective iterations scores were save to files for use by \autoref{code:q1r} to generate the respective iterations graph plot, the python code for this question can be seen in \autoref{code:q1p}. Please note that the score label for node seven in  \autoref{fig:q1_i1} through \autoref{fig:q1_i5} has a pagerank of 0.7 throughout. This happens due to the cutting short of the algorithms iterations which was necessary to answer this question.  

After the first iteration \autoref{fig:q1_i1} the scores for each node begin to show who are the good hubs or authorities. Node one and six have interesting values from the hits algorithm. Both nodes authority scores are the highest out of the others likewise with the pagerank score. The other nodes scores show that they are more hubs than authorities except node five which has all three scores present. Iterations two \autoref{fig:q1_i2} through five  \autoref{fig:q1_i5} show that only nodes five and one have value changes and all other node values stay the same. This leads me to believe that due to the limited size of this graph that it only took two iterations to determine the final graph. When only considering pagerank scores it would be hard to determine who was an authority, a hub or both. 

This can be seen in nodes five,three,two and four. Node five who is both an authority and a hub has a pagerank score of 0.11 whereas nodes three, two and four have pagerank scores of 0.07 but node three has a hub score of 0.45 unlike the others who with 0.2. These nodes pagerank score difference from node 5 is only 0.04 which is small enough that without prior knowledge of the graph using pagerank as an indicator for who was a hub or an authority would be hard.
\begin{figure}[H]
\centering
\includegraphics[scale=.8]{q1_iteration1.png}
\caption{Iteration 1}
\label{fig:q1_i1}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[scale=.8]{q1_iteration2.png}
\caption{Iteration 1}
\label{fig:q1_i2}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[scale=.8]{q1_iteration3.png}
\caption{Iteration 3}
\label{fig:q1_i3}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[scale=.8]{q1_iteration4.png}
\caption{Iteration 4}
\label{fig:q1_i4}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[scale=.8]{q1_iteration5.png}
\caption{Iteration 5}
\label{fig:q1_i5}
\end{figure}
\begin{code}
\captionof{listing}{Q1 Python} 
\label{code:q1p}
	\pycode{code/q1_10.3.py}
\end{code}
\begin{code}
\captionof{listing}{Q1 R} 
\label{code:q1r}
	\rcode{code/q1.R}
\end{code}
\newpage
\section{Question 10.6} \label{q2}
\begin{verbatim}
Find two examples of document filtering systems on the Web.  How do they build a profile for 
your information need?  Is the system static or adaptive?
\end{verbatim}
\subsection*{Answer} 
For my two examples I have choose Amazon and Spotify as both use an adaptive system for making recommendations to me.
\autoref{fig:q2_awhy} shows two snack food items I purchased together through Amazon both of which happen to be my all time favorite snack foods. 
\begin{figure}[H]
\centering
\adjustbox{trim={0} {3.1in} {0} {3cm},clip}%
{\includegraphics[scale=.8]{whyAmazonRecommened.png}}
\caption{Purchased Snack Foods}
\label{fig:q2_awhy}
\end{figure}
\noindent Ever since then I have been getting recommendations for similar snack foods as seen in \autoref{fig:q2_arec} which shows the first four recommendations. Both of the items I purchased had four and three fourth star ratings which was used in making the correlation to these recommended items \autoref{fig:q2_arec_ff}. Three of these recommendation are other Corn Nut products while the fourth is Jerky. This is included because other Amazon customers who bought Corn Nuts also purchased Jack Link's Beef Jerky with it \autoref{fig:q2_arec_bt}. This frequently bought item is also recommend for purchase with Corn Nuts when adding it to your cart.
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
    \includegraphics[scale=.6]{amazon_recp1.png}
     \includegraphics[scale=.6]{amazon_recp2.png}
          \caption{First Four Recommendations}
         \label{fig:q2_arec_ff}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
      \adjustbox{trim={0} {1.2cm} {3in} {1.3cm},clip}%
{\includegraphics[scale=.7]{amazonCornFeq.png}}
        \caption{Frequently Bought With Corn Nuts}
          \label{fig:q2_arec_bt}
    \end{subfigure}
       \caption{Amazon Snack Food Recommendations}
    \label{fig:q2_arec}
\end{figure}
\noindent Amazon uses your purchase history and correlates it with purchases from other users who have also bought the same item as you to build its recommendations. My second example Spotify is an online music streaming service that allows for user created playlists, similar artists recommendations, top chart playlists and auto created playlists for a user.  \autoref{fig:q2_recpl} shows this weeks auto created playlist for me but most of the songs listed I am not familiar with by name. It was not until I listened to a few that I recognized them as they come from other user created playlists which I subscribe to. They were recommend to me because other users have \enquote{liked} the track i.e hit a check mark beside it after which it was saved to their like song list. Most of them were not too bad but the 4ware track by deadmau5 is excellent and one I liked as well after hearing it again via this recommended playlist.
\begin{figure}[H]
\centering
\includegraphics[scale=.6]{spotify_discover2.png}
   \caption{Recommended Playlist}
          \label{fig:q2_recpl}
\end{figure}
Since Spotify uses your listen history in combination with other users plus liked tracks to make recommendations and the recommendation is music the results can be weird at times or way off. For instance \autoref{fig:q2_snails} which shows the first three recommendations for similar albums based off listening to Nails. Nails is ultra aggressive hardcore/powerviolence band so the recommendations for Weekend Nachos and Cult Leader makes sense as they are in the sludgy crust punk powerviolence spectrums. Now Oathbreaker on the other hand is a post-hardcore band that has some tracks that could be called hardcore punk but that is up for debate. Looks like a bunch of wayward post-hardcorers like Nails and is why Spotify included in this recommendation.
\begin{figure}[H]
\centering
      \adjustbox{trim={0} {0} {2in} {0},clip}%
{\includegraphics[scale=.5]{simToNails.png}}
   \caption{Suggestions Nails}
          \label{fig:q2_snails}
\end{figure}
\noindent As much as I would disagree with this recommendation it proves that Spotify is using an adaptive system to generate the recommendations it makes.
\clearpage
\section{Question 10.8} \label{q3}
\begin{verbatim}
Implement the nearest neighbor based collaborative filtering algorithm.
Using a publicly available collaborative filtering data set, compare the effective-
ness, in terms of mean squared error, of the Euclidean distance and correlation
similarity.
\end{verbatim}
\subsection*{Answer} 
I chose this question because it is a variation on assignment seven from cs532 Web Sciences and like that question I used the MoveLen dataset. 
But unlike that question I did not use the code submitted for it nor the code provided from the courses text book Programming Collective Intelligence. Instead I choose to use Pandas\footnote{\href{http://pandas.pydata.org/}{http://pandas.pydata.org/}} which is a high performance data analysis library akin to R. The source code used to generate the similarities can be seen in \autoref{code:q2cs} and \autoref{code:q2cs_help}.\newline 
\indent The MoveLen dataset contains 100,000 ratings (1-5) from 943 users for 1682 movies where each user has rated at least 20 movies. So computing the similarity for each user for a given k value would take some time as this user based similarity. In order to ensure the computation of k nearest neighbors  for an given k value for all users could be done relatively fast, the similarities for all users were computed, i.e 1 user vs 942 others 943 times. This was accomplished by processing 23 \enquote{\textit{chunks}} of 41 users 4 chunks at a time in parallel. Then for each user in a chunk the users similarity to every other user was computed using Euclidean distance and Pearson as the similarity measures. Once all chunks had been processed the results were combined into a single \ipy{pandas.DataFrame} and serialized to disk. This method of processing is much like the map and reduce paradigm. Instead of distributing the similarity computing function to multiple nodes pythons \ipy{multiprocess.Pool} applied that function to each chunk (map) using four different processes on my local machine and then combined (reduce) into a single result by the process which spawned them. \newline
\indent After the one vs all similarity for the users was computed the k nearest neighbors for $k=10$ was found for both similarity measures and \autoref{fig:q2_mse} show the mean squared error for both measures.
\begin{figure}[H]
\centering
\includegraphics[scale=.8]{user_predicted_mse.png}
\caption{Predicted MSE}
\label{fig:q2_mse}
\end{figure}
\clearpage
Overall Euclidean distance measure performed the best whereas Pearsons had more extremes. The trend line for both measures shows they behave similarly even tho Pearson had higher MSE values overall. This is more easily seen when considering \autoref{fig:q2_p} and \autoref{fig:q2_u} which show a histogram (bin width = 50) of the ratings for both measures compared to the predicted values. Pearson, \autoref{fig:q2_p} predicted values that were $\rpm 1$ ranking than the actual. Euclidean distance faired similarly \autoref{fig:q2_u} but less so. The reason behind this could be attributed to the fact it is more suited to a clustering algorithm since it is more geared to this kind of problem than a true similarity function.
\begin{figure}[H]
\centering
\includegraphics[scale=0.8]{pearson_predicted_buser.png}
\caption{User Prediction Pearson}
\label{fig:q2_p}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[scale=0.8]{euclid_predicted_buser.png}
\caption{User Prediction Euclidean Distance}
\label{fig:q2_u}
\end{figure}

\begin{code}
\captionof{listing}{Q2 Compute Similarities} 
\label{code:q2cs}
	\pycode{code/q2_10.8.py}
\end{code}
\begin{code}
\captionof{listing}{Q2 Generate Graphs} 
\label{code:q2rgg}
	\rcode{code/q2_plotter.R}
\end{code}
\begin{code}
\captionof{listing}{Q2 Compute Similarities Helper} 
\label{code:q2cs_help}
	\pycode{code/q2_helpers.py}
\end{code}
\begin{code}
\captionof{listing}{Q2 Generate Graphs Helper} 
\label{code:q2rgg_help}
	\rcode{code/q2_plotter_helper.R}
\end{code}
\clearpage
\section{Question 11.2} \label{q4}
\begin{verbatim}
Does your favorite web search engine use a bag of words representation?
How can you tell whether it does or doesn’t?
\end{verbatim}
\subsection*{Answer} 
Unlike the vast majority of the world my favorite search engine is \href{https://duckduckgo.com}{duckduckgo.com} because that's so met\anarchy{}l but in all seriousness I like it because it is a \href{https://duckduckgo.com/about}{privacy} first search engine and the \href{https://www.eff.org/deeplinks/2010/10/search-engines-protect-privacy-outbound-https}{EFF} likes them. All right consider the following query seen in \autoref{fig:q4_pv}. 
\begin{figure}[H]
\centering
\includegraphics[scale=0.8]{duckduckgo_pv.png}
\caption{Powerviolence Query}
\label{fig:q4_pv}
\end{figure}
The top three results, along with the others returned by this query, are about the best subgenre of punk powerviolence (pv). I purposely did not use the proper form of pv, as a single word, because in bag of words both \enquote{power} and \enquote{violence} would be in the \enquote{bag} when applying this retrieval model to build the result set. To better understand this consider the following two documents:
\begin{description}
\item[Doc1] Powerviolence (sometimes written as power violence) is a raw and dissonant subgenre of hardcore punk.
\item[Doc2] The violence today at the power plant was extreme
\end{description}
In the bag of words model the two documents would be transformed into: \newline \newline
[\enquote{Powerviolence}, \enquote{sometimes}, \enquote{written}, \enquote{as}, \enquote{power}, \enquote{violence}, \enquote{is}, \enquote{a}, \enquote{raw}, \enquote{and}, \enquote{dissonant}, \enquote{subgenre}, \enquote{of}, \enquote{hardcore}, \enquote{punk}, \enquote{The}, \enquote{violence}, \enquote{today}, \enquote{at}, \enquote{the}, \enquote{power}, \enquote{plant}, \enquote{was}, \enquote{extreme}] \newline \newline
Because of this my query would have \textbf{Doc2} returned in the result set which is not desired. To further test this I issued the query \textit{violence power} and the results can be seen in \autoref{fig:q4_vp}.
\begin{figure}[H]
\centering
\includegraphics[scale=0.8]{duckduckgo_vp.png}
\caption{Violence  Power Query}
\label{fig:q4_vp}
\end{figure}
The top three results for this query do not have anything to do about this subgenre of punk rather the typical expectation for this combination words was returned. But the Wikipedia article for powerviolence was at position ten of the result set with all other results not about powerviolence.
\clearpage
\section{Extra Credit Wiki Small} \label{q5}
\begin{verbatim}
Turn in with A5, 8 points available. Download the wikipedia.org URIs from the live web.

summarize the HTTP status codes returned,all 200? Redirections? 404s? 
do we still have 6043 documents? or have their been splits & merges 
 (see also: wikipedia disambiguation pages)? (1 pt)

compare and contrast the in- and out-links from this collection. the number, 
the domains linked to, the HTTP status of the non-wikipedia links in the article 
(i.e., are they 200, 404, or something else?). (1 pt)

Compare the least and most popular articles (in terms of in-links) in the 2008 snapshot 
vs. the least and most popular articles in your 2015 snapshot. (1 pt)

compare and contrast the anchor text for the articles in the collection: 
are we gaining or losing terms? (1 pt)

compare and contrast the sizes of the same pages (i.e., 2008 vs. 2015) in: (2 pts)
• bytes
• total terms
• total unique terms

for all of the pages (i.e., treat each collection as a single unit), 
compare the 2008 vs. 2015 snapshots in: (2pts)
• bytes
• total terms
• total unique terms
\end{verbatim}
\section*{Note}
All http requests made while answering this question had a maximum timeout of 5 seconds this was to ensure that a lone url would not hold up the parallelization used to speed up the processing. The main code for this can be seen in \autoref{code:wsmall}, its helper file \autoref{code:wsmall_help}, the vocab code \autoref{code:wsmall_vocab} and the R code used to generate the graphs \autoref{code:wsmall_plotter}. 
\subsection*{Wiki Small Statuses} 
% web.archive 4 timeouts
The statuses for the 6043 documents can be seen in \autoref{tbl:ws_http}. 
\begin{table}[H]
	\caption{Wiki Small Live Web Counterparts Http Statuses}
	\label{tbl:ws_http}
	\centering
\begin{tabular}{lr}
\hline
status & count \\
\hline
200 &  5800\\
404 &  243\\
\hline
\end{tabular}
\end{table}
\noindent As seen in this table the page was either existing or non-existing, those that were not deemed not existing could not be retrieved using the wiki file name (after sensitization) appended to  the following url https://en.wikipedia.org/wiki/. On looking up the pages on wikipedia itself some could be found again but the page url had changed but the majority of them had new pages containing the key words from the original. For example Saint-Hilaire\_Lange\_National\_Park now resides at Saint-Hilaire/Lange\_National\_Park, Humans\_United\_Against\_Robots has been merged into the page Keith and The Girl \autoref{fig:ws_l4041} and the Lakshmana\_Swamy page has been renamed to A. Lakshmanaswami Mudaliar but the original pages terms show up in multiple other pages \autoref{fig:ws_l4042}.
\begin{figure}[H]
\centering
     \adjustbox{trim={0} {0} {6in} {0},clip}%
{\includegraphics[scale=0.8]{wsmall_live2_404.png}}
\caption{Wsmall 404  Humans\_United\_Against\_Robots}
\label{fig:ws_l4041}
\end{figure}
\begin{figure}[H]
\centering
     \adjustbox{trim={0} {0} {6in} {0},clip}%
{\includegraphics[scale=0.8]{wsmall_live_404.png}}
\caption{Wsmall 404  Lakshmana\_Swamy}
\label{fig:ws_l4042}
\end{figure}
\subsection*{Wiki Small Live Web Pages Outlink Statuses} 
There were a total of 80,686 outlinks for the live web pages and of those links 68,338 unique, \autoref{tbl:ws_ohttp} shows the counts for all statuses. I was surprised to find that the live web wiki pages contained links to ip addressed 25 to be exact. An attempt to retrieve them was made but the requests library\footnote{http://docs.python-requests.org/en/master/} threw exceptions for all of them so they were skipped. Also to my surprise was the low number timeouts that happened during processing which was only 3,504.
\begin{table}[H]
	\caption{Wiki Small Live Web Counterparts Outlink Http Statuses}
	\label{tbl:ws_ohttp}
	\begin{minipage}{.5\linewidth}
		\centering
		\begin{tabular}{lr}
			\hline
			status   & count \\
			\hline
			ip       & 25    \\
			timedout & 3,504  \\
			200      & 42,315 \\
			203      & 1     \\
			204      & 2     \\
			300      & 1     \\
			301      & 12,606 \\
			302      & 3,423  \\
			303      & 2,253  \\
			307      & 29    \\
			400      & 35    \\
			401      & 36    \\
			403      & 242   \\
			\hline
		\end{tabular}
	\end{minipage} %
	\begin{minipage}{.5\linewidth}
		\centering
		\begin{tabular}{lr}
			\hline
			status & count \\
			\hline
			404      & 3,963  \\
			405    & 546   \\
			406    & 1     \\
			408    & 1     \\
			409    & 2     \\
			410    & 27    \\
			415    & 2     \\
			429    & 6     \\
			500    & 103   \\
			501    & 5     \\
			502    & 33    \\
			503    & 68    \\
			504    & 1     \\
			520    & 15    \\
			\hline
		\end{tabular}
	\end{minipage} 
\end{table}
During the retrieval of these statuses I noticed a good number of urls were to web archives, so naturally I had to find out how healthy these links are. \autoref{tbl:ws_archives} shows the total number of uri-ms for each archive. Please note that archive.org's count is different than web.archive.org's as archive.org pointed to archive collection where as the latter was a true uri-m. The archive with the most dominate presence was web.archive.org with 2,049 uri-ms which is 2.5\% of the total out bound links.
\begin{table}[H]
	\caption{Web Archives Represented in Live Web Wiki Small Outlinks}
	\label{tbl:ws_archives}
	\centering
\begin{tabular}{lrr}
	\hline
	Archive                            & Count & Percent Of Total Non-Wiki Outlinks \\
	\hline
	archive.org                        &     263 & 0.325955\%                   \\
 archive.today                      &       1 & 0.001239\%                   \\
 britishnewspaperarchive.co.uk      &       5 & 0.006197\%                   \\
 web.archive.org                    &    2,049 & 2.539474\%                   \\
 webarchive.loc.gov                 &       2 & 0.002479\%                   \\
 webarchive.nationalarchives.gov.uk &       7 & 0.008676\%                   \\
 webcitation.org                    &     269 & 0.333391\%                   \\
	\hline
\end{tabular}
\end{table}
Sadly as seen in \autoref{tbl:ws_archives_stat} not all uri-ms are healthy but most of the non 200 are 3xx so their may be hope.
\begin{table}[H]
	\caption{Web Archives Outlink Statuses}
	\label{tbl:ws_archives_stat}
		\begin{minipage}{.5\linewidth}
	\centering
	\begin{tabular}{lrr}
\hline
 Archive                            &   Status &   Count \\
\hline
  archive.org &      200 &     235 \\
 archive.org &      301 &       1 \\
 archive.org &      302 &       4 \\
 archive.org &      403 &       1 \\
 archive.org &      404 &       6 \\
  archive.today &      301 &       1 \\
 britishnewspaperarchive.co.uk      &      302 &       5 \\
\hline
\end{tabular}
	\end{minipage} 
	\begin{minipage}{.5\linewidth}
		\centering
		\begin{tabular}{lrr}
\hline
 Archive                            &   Status &   Count \\
\hline
 web.archive.org &      200 &    1,863 \\
 web.archive.org &      302 &      78 \\
 web.archive.org &      400 &       3 \\
 web.archive.org &      403 &       9 \\
 web.archive.org &      404 &      22 \\
 webarchive.loc.gov                 &      200 &       2 \\
 webarchive.nationalarchives.gov.uk &      200 &       7 \\
 webcitation.org &      200 &     258 \\
\hline
\end{tabular}
	\end{minipage} 
\end{table}
Also only 101 time outs where encountered during the uri-m health check as seen in table \autoref{tbl:ws_archives_stat_to}.
\begin{table}[H]
	\caption{Web Archives Outlinks  Timed Out Count}
	\label{tbl:ws_archives_stat_to}
	\centering
\begin{tabular}{lr}
\hline
 Archive                            &   Times Connection Timed Out \\
\hline
 archive.org                        &               16 \\
 web.archive.org                    &             74\\
 webcitation.org                    &               11 \\
\hline
\end{tabular}
\end{table}
\subsection*{Wiki Small Dataset vs Live Web Counterparts File Sizes} 
\autoref{fig:ws_fdif} shows the file size differences for all 5800 live web counterparts. The difference was calculated by subtracting the live webs file size from the datasets. It must be noted that the dataset versions were pre-sanatized to remove all extra markup (including text not about the article) and given the age differences the dataset versions do not include any internal changes that wikipedia made to the pages representation.
\begin{figure}[H]
\centering
\includegraphics[scale=0.8]{wikismall_dif.png}
\caption{Wiki Small File Differences From Live Web Counterparts}
\label{fig:ws_fdif}
\end{figure}
\subsection*{Wiki Small Dataset vs Live Web Counterparts Vocabulary Growth} 
Since the live web files contain more mark up and text than the datasets I simply plotted the two counts using the code for assignment 2 \autoref{fig:ws_vocab} shows this graph.
\begin{figure}[H]
\centering
\includegraphics[scale=0.8]{wikismall_vocab_compare.png}
\caption{Wiki Small Vocab Growth Comparison}
\label{fig:ws_vocab}
\end{figure}
\begin{code}
\captionof{listing}{Wiki Small Code} 
\label{code:wsmall}
	\pycode{code/wikiq.py}
\end{code}
\begin{code}
\captionof{listing}{Wiki Small Helper Code} 
\label{code:wsmall_help}
	\pycode{code/wiki_helpers.py}
\end{code}
\begin{code}
\captionof{listing}{Wiki Small Vocab} 
\label{code:wsmall_vocab}
	\pycode{code/wiki_vocab.py}
\end{code}
\begin{code}
\captionof{listing}{Wiki Small Plotter} 
\label{code:wsmall_plotter}
	\rcode{code/wiki_plotter.py}
\end{code}
\newpage
\begin{code}
\captionof{listing}{General Utility Functions} 
\label{code:util}
	\pycode{code/util.py}
\end{code}
\begin{code}
\captionof{listing}{Modified NX Hits and Pagerank Algos} 
\label{code:nxc}
	\pycode{code/nx_controled_algos.py}
\end{code}
\begin{code}
\captionof{listing}{Updated Context Classes} 
\label{code:cntxc}
	\pycode{code/contextClasses.py}
\end{code}
\end{document}